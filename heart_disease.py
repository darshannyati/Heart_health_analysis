# -*- coding: utf-8 -*-
"""heart_disease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jASx1y7P8uoX2yFxxuv-0uxB5uP60Gjp
"""

# Commented out IPython magic to ensure Python compatibility.
# Import all the tools we need

# Regular EDA (exploratory data analysis) and plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# we want our plots to appear inside the notebook
# %matplotlib inline

# Models from Scikit-Learn
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
import sklearn



# Model Evaluations
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("heart.csv")
df.shape

df.head()

"""age - age in years
sex - 1 = male; 0 = female
cp - chest pain type
0: Typical angina: chest pain related decrease blood supply to the heart
1: Atypical angina: chest pain not related to heart
2: Non-anginal pain: typically esophgael spasms (non heart related)
3: Asymptomatic: chest pain not showing signs of disease
trestbps - resting blood pressure (in mm Hg on admission to the hospital)
chol - serum cholestoral in mg/dl
serum = LDL + HDL + .2 * triglycerides
above 200 is cause for concern
fbs(fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
'>126' mg/dL signals diabetes
restecg - resting electrocardiographic results
0: Nothing to note
1: ST-T Wave abnormality
can range from mild symptoms to severe problems
signals non-normal heart beat
2: Possible or definite left venticular hypertrophy
Enlarged heart's main pumping chamber
thalach - maximum heart rate achieved
exang - exercise induced angina (1 = yes; 0 = no)
oldpeak - ST depression induced by exercise relative to rest
looks at stress of heart during excercise
unhealthy heart will stress more
slope - the slope of the peak exercise ST segment
0: Unsloping: better heart rate with excercise (uncommon)
1: Flatsloping: minimal change (typical healthy heart)
2: Downsloping: Signs of unhealthy heart
ca - number of major vessels (0-3) colored by flourosopy
colored vessel means the doctor can see the blood passing through
the more blood movement the better (no clots)
thal - thalium stress result
1,3 = normal
6 = fixed defect: Used to be defect but ok now
7 = reversable defect: no proper blood movement when excercising
target - have disease or not (1=yes, 0=no)(= the predicted attribute
# \
"""

df.tail()

df.target.value_counts()

df.target.value_counts(normalize=True)

df.target.value_counts().plot(kind="bar", color=["salmon", "lightblue"]);

df.info()

df.describe()

df.isna().sum()

df.sex.value_counts()

pd.crosstab(df.target, df.sex)

# Create a plot of crosstab
pd.crosstab(df.target, df.sex).plot(kind="bar",
                                    figsize=(10, 6),
                                    color=["salmon", "lightblue"])

plt.title("Heart Disease Frequency for Sex")
plt.xlabel("0 = No disease, 1 = disease")
plt.ylabel("Amount")
plt.legend(["Female", "Male"])
plt.xticks(rotation=0)

# Create another figure
plt.figure(figsize=(10, 6))

# Scatter with possitive examples
plt.scatter(df.age[df.target == 1], df.thalach[df.target == 1], c="salmon")

# Scatter with negative examples
plt.scatter(df.age[df.target == 0], df.thalach[df.target == 0], c="lightblue")

# Add some helpful info
plt.title("Heart disease in function of Age and Max Heart Rate")
plt.xlabel("Age")
plt.ylabel("Max Heart Rate")
plt.legend(["disease", "No Disease"])

df.age.plot.hist();

pd.crosstab(df.cp, df.target)

# Make the crosstab more visual
pd.crosstab(df.cp, df.target).plot(kind="bar",
                                   figsize=(10, 6),
                                   color=["salmon", "lightblue"])

# Add some communication
plt.title("Heart Disease Frequency per Chest Pain Type")
plt.xlabel("Chest pain Type")
plt.ylabel("Amount")
plt.legend(["No Disease", "Disease"])
plt.xticks(rotation=0)

# Find the correlation between our independent variables
corr_matrix = df.corr()
corr_matrix

# let's make our correlation matrix a little prettier
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15, 10))
ax = sns.heatmap(corr_matrix,
                 annot=True,
                 linewidths=0.5,
                 fmt=".2f",
                 cmap="YlGnBu")

# Everything except target variable
X = df.drop("target", axis=1)

# Target variable
y = df["target"]

# Independent variables (no target column)
X

# Targets
y

# Random seed for reproducibility
np.random.seed(42)

# Split into train and test set
X_train, X_test, y_train, y_test = train_test_split(
    X,  #independent variables
    y,  # dependent variable
    test_size=0.2)  # percentage of data to use for test set

X_train.head()

np.array(y_train), len(y_train)

np.array(y_test), len(y_test)

# Put models in a dictionary
models = {
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier()
}


# Create function to fit and score models
def fit_and_score(models, X_train, X_test, y_train, y_test):
    """
    Fits and evaluates given machine learning models.
    models : a dict of different Scikit-Learn machine learning models
    X_train : training data
    X_test : testing data
    y_train : labels assosciated with training data
    y_test : labels assosciated with test data
    """
    # Random seed for reproducible results
    np.random.seed(42)
    # Make a list to keep model scores
    model_scores = {}
    # Loop through models
    for name, model in models.items():
        # Fit the model to the data
        model.fit(X_train, y_train)
        # Evaluate the model and append its score to model_scores
        model_scores[name] = model.score(X_test, y_test)
    return model_scores

model_scores = fit_and_score(models=models,
                            X_train=X_train,
                            X_test=X_test,
                            y_train=y_train,
                            y_test=y_test)

model_scores

model_compare = pd.DataFrame(model_scores, index=["accuracy"])
model_compare.T.plot.bar();

sns.set(font_scale=1.5)

def plot_conf_mat(y_test, y_preds):
  """
  Plots a nice looking confusion matrix using Seaborn's heatmap()
  """
  fig, ax = plt.subplots(figsize=(3, 3))
  ax = sns.heatmap(confusion_matrix(y_test, y_preds),
                   annot=True,
                   cbar=False,
                   fmt='g') # modified to show numbers instead of le+02
  plt.xlabel("True label")
  plt.ylabel("Predicted label")

plot_conf_mat(y_test, y_preds)



np.array(y_test)

print(classification_report(y_test, y_preds))

#  convert the above data in bar graph

# ... (preceding code)

# Extract values from classification report
report = classification_report(y_test, y_preds, output_dict=True)
metrics = ['precision', 'recall', 'f1-score']
classes = ['0', '1']

# Create data for bar plot
data = []
for metric in metrics:
    for class_ in classes:
        data.append({'Metric': metric, 'Class': class_, 'Value': report[class_][metric]})

# Convert to DataFrame for plotting
df_metrics = pd.DataFrame(data)

# Create bar plot
plt.figure(figsize=(10, 6))
sns.barplot(x='Metric', y='Value', hue='Class', data=df_metrics)
plt.title('Classification Report Metrics')
plt.ylabel('Score')
plt.show()

#  apply cross validation on our random forest model to improve its accuracy

# Random seed for reproducibility
np.random.seed(42)

# Instantiate Random Forest Classifier
clf = RandomForestClassifier()

# Perform cross-validation
cv_acc = cross_val_score(clf, X, y, cv=5, scoring="accuracy")

# Print cross-validation scores
print(f"Cross-validated accuracy scores: {cv_acc}")
print(f"Mean cross-validated accuracy: {np.mean(cv_acc) * 100:.2f}%")

# Based on our analysis, the Random Forest Classifier model demonstrated the highest accuracy score among the tested models (KNN, Logistic Regression, Random Forest).
# However, it's crucial to consider the context of medical diagnosis and the potential consequences of misclassification.

# The confusion matrix and classification report provide insights into the model's performance on different classes. It's important to analyze metrics like precision, recall, and F1-score,
# especially for imbalanced datasets, to understand the trade-offs between false positives and false negatives.

# Cross-validation helps assess the model's generalizability and robustness. The mean cross-validated accuracy provides a more reliable estimate of the model's performance on unseen data.

# Further improvements could involve hyperparameter tuning using techniques like GridSearchCV or RandomizedSearchCV to optimize the model's performance.
# Additionally, exploring other algorithms and feature engineering techniques might lead to better result.